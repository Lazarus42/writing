
I spoke with my brother yesterday about a book by Eric Helland and Alexander Tabarrok explaining the rising costs in healthcare and education. They explain the rising costs in terms of the Baumol effect. Imagine a fictitious economy that produces only cars and computers. In the 1900s, both are difficult to produce, computers more so, and there aren't many of either. In 2024, the automobile and computer industries have experienced massive growth and are significantly cheaper. The productivity increase in computers has far outpaced that of cars, though. Compared to the 1900s, cars are relatively more expensive. Explained in another way, now you need to give up many more computers to buy an extra car than you did before; in the 1950s you would need to give up multiple cars to get a computer! This is the Baumol effect. As interesting as Helland and Tabarrok's book is, it's only incidentally related to the topic of this essay. 

This essay is about opinions, how we should hold them, and how we can verify them. In conversation with my brother, I realized that I had a bunch of opinions injected into my mind from that book that I hadn't really thought hard about or verified. But, still, I believed them pretty strongly! If an opinion is like an onion where the top layer is the opinion itself, the core is a set of facts about reality, and intermediate layers are connections between the facts and the opinion, then I couldn't go more than a few, shallow layers deep in my explanations. 

Most opinions are hollow onions - there's an opinion there, connections to observations and other opinions, but there's no clear path to the ground reality. This probably holds for most opinions of most people! I try to think well, and a lot of my identity is being someone who aims for the truth, so this realization frightened me. If my thoughts are based primarily on my opinions, which are likely wrong, that means that there's a good chance that most conclusions I draw about the world are wrong, too. 

So, how can we fill in our opinions? A good first step is to internalize that they are hollow. It's hard to find what you're not looking for, and consequently, it's unlikely that you'll be rigorous in evaluating and forming opinions unless you try. The proof for this is, again, that most people's opinions on most things are wrong. 

Another question we need to address is how we can know if an opinion is hollow. A good test for this is writing down the opinion, the reasons we believe it, and iteratively justifying those reasons until finally reaching a set of facts. In essence, writing down the opinion's anatomy. As you write down reasons and connections, you might see that an implication is loose or entirely incorrect. You might see that the facts at the core are no longer true. If this happens, congratulations! You've discovered your opinion is hollow. You can now properly update the opinion by replacing its core, and fixing implications. An emergent fact of this exercise is that holding solid opinions requires a lot of effort. It is very difficult to "accidentally" hold solid opinions, and evidently, most opinions will not pass the anatomy test. 

An interesting consequence of the anatomy test is that most content we consume is entertainment. That podcast you listened to, that book you read, that conversation you had, the tweet you saw, all of these likely changed your opinions in a way that would not pass the anatomy exam. This is surprising because people like to view long-form intellectual content as learning. After all, well-respected podcasters and interviewers speak with world-renowned experts in their fields. While what they are saying may be true, automatically agreeing with their take and adopting their opinions without further work is dangerous. It's common, increasingly common actually, to hold a bunch of opinions from successful tech entrepreneurs, famous philosophers, and contrarian thinkers all while failing the anatomy exam miserably. What's more is that a lot of smart people fall victim to this while receiving social validation. In tech circles, for example, Peter Thiel is recognized as a contrarian genius. After a podcast appearance, smart, ambitious tech people will adopt his opinions and parrot them on Twitter and to their friends. Intellectual praise pours in and these people will feel they hold a deep, misunderstood truth. While what they're saying can be true, they have no idea why. The outside of their opinion looks healthy but peeling back the layers reveals an empty core. 

A natural follow-up question is - How can we possibly have only solid opinions if forming them takes so much effort? The premise here being that forming solid opinions takes a time, and a lot of it. Well, it's unlikely that we actually can have only solid opinions. It turns out this is probably not the right question to ask. A more interesting question is what can we do given that forming solid opinions is hard and effortful? The answer to this is probably to be extremely curious about the truth and to hold opinions loosely. If we are curious about the truth, and we seek it out constantly, we're going to have a good influx of information to update and form beliefs. And it's not going to feel like work. If it doesn't feel like work, which it won't because curiosity is self-rewarding, it'll happen all the time. So curiosity provides a great platform to update and form beliefs based on current reality. This won't do any good, though, if we resist using the platform. The second piece of the puzzle, holding opinions loosely, patches this. If we are curious, we have the chance to update and form solid beliefs, and if we hold our opinions loosely, we take that chance. Just as curiosity alone was insufficient, so is the case with loose opinions. Without curiosity, you're probably not getting enough exposure to the truth. The drawing below illustrates this point; non-stubborn incurious people probably hold more truthful opinions than stubborn incurious people, but the difference is probably pretty small. Another way to reach this conclusion is to accept that incorrect beliefs are usually about non-obvious things. They're non-obvious so they take a bit of thinking (work to the incurious) to understand. So non-obvious truths are equally mysterious to the non-stubborn incurious and the stubborn incurious. 

If most forms of content we consume is entertainment, then why put in the effort to read? 

Questions to consider:
- is reading mostly entertainment?
- thought ----- if we are bound to parrot what we see and hear to some extent, we should aim to at least have better content (more likely to be correct) in our minds. Quality books are more likely to have that. 
- related -- we are always going to have to base our opinions to some extent on external factors. We want those to be the best possible. That's why we choose high quality sources (related to choosing high quality sources when we write)
- another benefit of reading especially over more passive forms of content is we are more likely to take breaks, notes, and question what we're consuming. it's more difficult to npc through reading than television. 



Suggestions:

- give a personal example, how i applied it to update an opinion
- address some issues: what about when the truth is unclear? what about the fact that it's too time intensive to hold only valid opinions? 
- also want to address that most predictions about the future are probably wrong because of this (and most predictive models reasoned through by people are probably wrong)
- other practical solutions: ground your opinions in relevant data whenever possible. get your hands dirty with the hard data and replicate studies. try to accept a minimal amount of information just because.